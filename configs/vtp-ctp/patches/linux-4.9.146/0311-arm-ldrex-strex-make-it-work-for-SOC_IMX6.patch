From 14f4d6ffc61ae45469945806faa597aa01ca11f2 Mon Sep 17 00:00:00 2001
From: Oleg Karfich <oleg.karfich@wago.com>
Date: Tue, 22 Jan 2019 12:11:13 +0100
Subject: [PATCH] arm: ldrex/strex: make it work for SOC_IMX6

Commit '9e44d00 arm: disable most usage of ldrex/strex' from Sebastian Andrzej
Siewior is a workaround that was created for AM35xx SoCs only which
unfortunately at the moment is also enabled for AM335x SoCs.

The workaround does not work on IMX6 whichs need SMP to work. So we disabled it
by CONFIG switch.

Signed-off-by: Oleg Karfich <oleg.karfich@wago.com>
Reviewed-by: Heinrich Toews <heinrich.toews@wago.com>
---
 arch/arm/include/asm/atomic.h   | 2 +-
 arch/arm/include/asm/cmpxchg.h  | 9 +++++----
 arch/arm/include/asm/spinlock.h | 4 +++-
 arch/arm/lib/bitops.h           | 2 +-
 4 files changed, 10 insertions(+), 7 deletions(-)

diff --git a/arch/arm/include/asm/atomic.h b/arch/arm/include/asm/atomic.h
index 8d8eb6b..da37876 100644
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@ -30,7 +30,7 @@
 #define atomic_read(v)	READ_ONCE((v)->counter)
 #define atomic_set(v,i)	WRITE_ONCE(((v)->counter), (i))
 
-#if 0
+#if defined(CONFIG_SOC_IMX6)
 
 /*
  * ARMv6 UP and SMP safe atomic ops.  We use load exclusive and
diff --git a/arch/arm/include/asm/cmpxchg.h b/arch/arm/include/asm/cmpxchg.h
index 63ce6f2..9b1f8e47 100644
--- a/arch/arm/include/asm/cmpxchg.h
+++ b/arch/arm/include/asm/cmpxchg.h
@@ -5,7 +5,8 @@
 #include <linux/prefetch.h>
 #include <asm/barrier.h>
 
-#if defined(CONFIG_CPU_SA1100) || defined(CONFIG_CPU_SA110) || 1
+#if defined(CONFIG_CPU_SA1100) || defined(CONFIG_CPU_SA110) \
+	|| !defined(CONFIG_SOC_IMX6)
 /*
  * On the StrongARM, "swp" is terminally broken since it bypasses the
  * cache totally.  This means that the cache becomes inconsistent, and,
@@ -31,14 +32,14 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
 #ifdef swp_is_buggy
 	unsigned long flags;
 #endif
-#if 0
+#if defined(CONFIG_SOC_IMX6)
 	unsigned int tmp;
 #endif
 
 	prefetchw((const void *)ptr);
 
 	switch (size) {
-#if 0
+#if defined(CONFIG_SOC_IMX6)
 #ifndef CONFIG_CPU_V6 /* MIN ARCH >= V6K */
 	case 1:
 		asm volatile("@	__xchg1\n"
@@ -120,7 +121,7 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
 
 #include <asm-generic/cmpxchg-local.h>
 
-#if 1
+#if !defined(CONFIG_SOC_IMX6)
 /* min ARCH < ARMv6 */
 
 #ifdef CONFIG_SMP
diff --git a/arch/arm/include/asm/spinlock.h b/arch/arm/include/asm/spinlock.h
index d1684f9..3badd09 100644
--- a/arch/arm/include/asm/spinlock.h
+++ b/arch/arm/include/asm/spinlock.h
@@ -5,7 +5,9 @@
 #error SMP not supported on pre-ARMv6 CPUs
 #endif
 
-#error Can't use spinlocks
+#if !defined(CONFIG_SOC_IMX6)
+#error cant use spinlocks
+#endif
 
 #include <linux/prefetch.h>
 #include <asm/barrier.h>
diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index e017860..ab59470 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,7 +1,7 @@
 #include <asm/assembler.h>
 #include <asm/unwind.h>
 
-#if 0
+#if defined(CONFIG_SOC_IMX6)
 	.macro	bitop, name, instr
 ENTRY(	\name		)
 UNWIND(	.fnstart	)
-- 
2.7.4

